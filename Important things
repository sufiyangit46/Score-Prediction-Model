
##THIS IS USED INSIDE INITIATE MODEL TRAINER FOR HYPERPARAMETER TUNING TO FIND THE BEST SETTING FOR MODEL AND GIVE EXACT ALGORITHM, BEST HYPERPARAMETER, BETTER ACCURACY, BETTER R2 SCORE AND SAVED MODEL WITH TUNING.


def initiate_model_trainer(self, train_array, test_array):
    try:
        logging.info("Splitting training and test input data")
        X_train, y_train, X_test, y_test = (
            train_array[:, :-1],
            train_array[:, -1],
            test_array[:, :-1],
            test_array[:, -1],
        )

        # Models
        models = {
            "Random Forest": RandomForestRegressor(),
            "Decision Tree": DecisionTreeRegressor(),
            "Gradient Boosting": GradientBoostingRegressor(),
            "Linear Regression": LinearRegression(),
            "K-Neighbors Classifier": KNeighborsRegressor(),
            "XGBClassifier": XGBRegressor(),
            "CatBoosting Classifier": CatBoostRegressor(verbose=False),
            "AdaBoost Classifier": AdaBoostRegressor(),
        }

        # Hyperparameter Grids
        params = {
            "Random Forest": {
                "n_estimators": [50, 100, 200],
                "max_depth": [5, 10, 20],
            },
            "Decision Tree": {
                "max_depth": [5, 10, 15],
                "criterion": ["squared_error", "friedman_mse"],
            },
            "Gradient Boosting": {
                "learning_rate": [0.01, 0.05, 0.1],
                "n_estimators": [100, 200],
            },
            "Linear Regression": {},
            "K-Neighbors Classifier": {
                "n_neighbors": [3, 5, 7],
                "weights": ["uniform", "distance"],
            },
            "XGBClassifier": {
                "learning_rate": [0.01, 0.05, 0.1],
                "n_estimators": [100, 200],
                "max_depth": [3, 6, 9],
            },
            "CatBoosting Classifier": {
                "depth": [4, 6, 10],
                "learning_rate": [0.01, 0.05, 0.1],
            },
            "AdaBoost Classifier": {
                "learning_rate": [0.01, 0.05, 0.1],
                "n_estimators": [50, 100, 200],
            },
        }

        model_report = {}

        # Perform Grid Search for each model
        for model_name, model in models.items():
            para = params[model_name]

            gs = GridSearchCV(model, para, cv=3, n_jobs=-1, verbose=0)
            gs.fit(X_train, y_train)

            model.set_params(**gs.best_params_)
            model.fit(X_train, y_train)

            y_pred = model.predict(X_test)
            score = r2_score(y_test, y_pred)

            model_report[model_name] = score

        # Get best model
        best_model_score = max(model_report.values())
        best_model_name = list(model_report.keys())[
            list(model_report.values()).index(best_model_score)
        ]

        best_model = models[best_model_name]

        logging.info(f"Best Model: {best_model_name}, Score: {best_model_score}")

        save_object(
            file_path=self.model_trainer_config.trained_model_file_path,
            obj=best_model,
        )

        return best_model_score

    except Exception as e:
        raise CustomException(e, sys)
